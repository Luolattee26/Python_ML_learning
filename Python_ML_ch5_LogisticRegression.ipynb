{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :])\n",
    "clf.score(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEntropy(Self build version, IIS&~~DFP~~ algorithms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出现了莫名错误不知道为啥不能把IIS算法和DFP算法合并在一个class里面......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running IIS/GIS\n",
      "this tool use a special way to get more eigenfunction\n",
      "实际迭代次数200\n",
      "81.57894736842105\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.optimize import fminbound\n",
    "import copy\n",
    "\n",
    "\n",
    "class MaxEnt:\n",
    "    def __init__(self, epsilon=1e-4, maxstep=200, algorithms='', verbose=False):\n",
    "        print('this tool use a special way to get more eigenfunction')\n",
    "        self.epsilon = epsilon\n",
    "        self.maxstep = maxstep\n",
    "        self.w = None  # 特征函数的权重，其实就是omega\n",
    "        self.labels = None  # 标签\n",
    "        self.fea_list = []  # 特征函数\n",
    "        # 这里用了defaultdict这个方法，其实就是为字典设置了一个默认值，这样就防止了从字典里面按照key取值key不存在而出错的情况\n",
    "        self.px = defaultdict(lambda: 0)  # 经验边缘分布概率\n",
    "        self.pxy = defaultdict(lambda: 0)  # 经验联合分布概率\n",
    "        self.exp_fea = defaultdict(lambda: 0)  # 每个特征的期望\n",
    "        self.data_list = []  # 样本集，元素为tuple((X),y)\n",
    "        self.N = None  # 样本总量\n",
    "        self.M = None  # 某个训练样本包含特征的总数，这里假设每个样本的M值相同，即M为常数。\n",
    "        self.zw = None\n",
    "        self.n_fea = None  # 特征函数的总数\n",
    "        self.n_ilter = None # 实际的迭代次数\n",
    "        self._algorithms = algorithms # 选算法\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def init_param(self, X_data, y_data):\n",
    "        # 根据传入的数据集(数组)初始化模型参数\n",
    "        self.N = X_data.shape[0]\n",
    "        self.labels = np.unique(y_data)\n",
    "\n",
    "        self.fea_func(X_data, y_data) # 往fea_list里面放进了需要的特征函数\n",
    "        self.n_fea = len(self.fea_list)\n",
    "        self.w = np.zeros(self.n_fea) # 初始化w值\n",
    "        self._exp_fea(X_data, y_data) # 往exp_fea放进了计算得到的exp_fea\n",
    "        return\n",
    "\n",
    "    def fea_func(self, X_data, y_data, rules=None):\n",
    "        # 这里特征函数的构造方法很巧妙\n",
    "        # 特征函数\n",
    "        if rules is None:  # 若没有特征提取规则，则直接构造特征，此时每个样本没有缺失值的情况下的特征个数相同，等于维度\n",
    "            for X, y in zip(X_data, y_data):\n",
    "                X = tuple(X)\n",
    "                self.px[X] += 1.0 / self.N  # X的经验边缘分布\n",
    "                self.pxy[(X, y)] += 1.0 / self.N  # X,y的经验联合分布\n",
    "\n",
    "                for dimension, val in enumerate(X):\n",
    "                    key = (dimension, val, y)\n",
    "                    if not key in self.fea_list:\n",
    "                        self.fea_list.append(key)  # 特征函数，由 (维度+维度下的值+标签) 构成的元组\n",
    "            self.M = X_data.shape[1]\n",
    "        else:\n",
    "            self.M = defaultdict(int)  # 字典存储每个样本的特征总数，defaultdict会自动构造一个字典\n",
    "            for i in range(self.N):\n",
    "                self.M[i] = X_data.shape[1]\n",
    "            pass  # 根据具体规则构建，因为这里并没有具体规则，所以就写了个pass\n",
    "\n",
    "    def _exp_fea(self, X_data, y_data):\n",
    "        # 计算特征的经验期望值\n",
    "        for X, y in zip(X_data, y_data):\n",
    "            for dimension, val in enumerate(X):\n",
    "                fea = (dimension, val, y)\n",
    "                self.exp_fea[fea] += self.pxy[(tuple(X), y)]  # 上面fea_func里面算过了，这里只是取出来，用特征函数的形式存放起来\n",
    "        return\n",
    "\n",
    "    def _py_X(self, X):\n",
    "        # 当前w下的条件分布概率,输入向量X和y的条件概率\n",
    "        py_X = defaultdict(float)\n",
    "        \n",
    "        # 这里计算py_x以及计算规范化因子的方法也很牛逼......\n",
    "\n",
    "        for y in self.labels:\n",
    "            s = 0\n",
    "            for dimension, val in enumerate(X):\n",
    "                tmp_fea = (dimension, val, y)\n",
    "                for i in self.fea_list:\n",
    "                    if tmp_fea == i:  # 输入X包含的特征\n",
    "                        s += self.w[self.fea_list.index(tmp_fea)] # 因为特征函数对于存在的特征直接是1，所以这里只需要self.w就行\n",
    "            py_X[y] = np.exp(s)\n",
    "\n",
    "        normalizer = sum(py_X.values()) # 这个normalizer就是规范化因子，好牛逼的方法......\n",
    "        self.zw = normalizer # 保存一下zw\n",
    "        for key, val in py_X.items():\n",
    "            py_X[key] = val / normalizer\n",
    "        return py_X\n",
    "\n",
    "    def _est_fea(self, X_data, y_data):\n",
    "        # 基于当前模型，获取每个特征估计期望\n",
    "        est_fea = defaultdict(float)\n",
    "        for X, y in zip(X_data, y_data):\n",
    "            py_x = self._py_X(X)[y]\n",
    "            for dimension, val in enumerate(X):\n",
    "                est_fea[(dimension, val, y)] += self.px[tuple(X)] * py_x\n",
    "        return est_fea\n",
    "\n",
    "    def GIS(self, X_data, y_data):\n",
    "        # GIS算法更新delta\n",
    "        # 通用迭代算法（这算法好像用的很少）\n",
    "        # 算法具体解释参考：https://blog.csdn.net/u014688145/article/details/55003910\n",
    "        # 这里的delta就是参数w每次的迭代变化值\n",
    "        est_fea = self._est_fea(X_data, y_data)\n",
    "        delta = np.zeros(self.n_fea)\n",
    "        # 这是只写了一次迭代\n",
    "        for j in range(self.n_fea):\n",
    "            try:\n",
    "                delta[j] = 1 / self.M * math.log(self.exp_fea[self.fea_list[j]] / est_fea[self.fea_list[j]])\n",
    "            except:\n",
    "                continue\n",
    "        delta = delta / delta.sum()  # 归一化，防止某一个特征权重过大导致，后续计算超过范围\n",
    "        # 这个感觉有点意义不明\n",
    "        return delta\n",
    "\n",
    "    def IIS(self, delta, X_data, y_data):\n",
    "        # IIS算法更新delta\n",
    "        # 当self.M为常数的时候，IIS/GIS两个算法完全等价，delta也存在显式解（统计学习书上有）\n",
    "        g = np.zeros(self.n_fea)\n",
    "        g_diff = np.zeros(self.n_fea)\n",
    "        for j in range(self.n_fea):\n",
    "            for k in range(self.N):\n",
    "                # 这下面的公式其实是统计学习书上推导的，实际上就是牛顿法\n",
    "                # 不过这个迭代公式似乎省略了一个exp_fea，但是从迭代的角度来看好像完全是不影响的\n",
    "                g[j] += self.px[tuple(X_data[k])] * self._py_X(X_data[k])[y_data[k]] * math.exp(delta[j] * self.M)\n",
    "                g_diff[j] += self.px[tuple(X_data[k])] * self._py_X(X_data[k])[y_data[k]] * math.exp(delta[j] * self.M) * self.M\n",
    "            g[j] -= self.exp_fea[j]\n",
    "            delta[j] -= g[j] / g_diff[j]\n",
    "        return delta\n",
    "\n",
    "\n",
    "    def fit(self, X_data, y_data):\n",
    "        self.n_ilter = 0\n",
    "        # 训练，迭代更新wi\n",
    "        self.init_param(X_data, y_data)\n",
    "        \n",
    "        # 选择算法\n",
    "        if self._algorithms == '':\n",
    "            if isinstance(self.M, int): # 判断self.M是不是一个常数，如果有自定义的特征提取规则，那会是个字典，不是int\n",
    "                i = 0\n",
    "                while i < self.maxstep:\n",
    "                    self.n_ilter += 1\n",
    "                    i += 1\n",
    "                    delta = self.GIS(X_data, y_data)\n",
    "                    if max(abs(delta)) < self.epsilon:  # 所有的delta都小于阈值时，停止迭代\n",
    "                        break\n",
    "                    self.w += delta\n",
    "            else:\n",
    "                i = 0\n",
    "                delta = np.random.rand(self.n_fea)\n",
    "                while i < self.maxstep:\n",
    "                    self.n_ilter += 1\n",
    "                    i += 1\n",
    "                    delta = self.IIS(delta, X_data, y_data)\n",
    "                    if max(abs(delta)) < self.epsilon:\n",
    "                        break\n",
    "                    self.w += delta   \n",
    "        elif self._algorithms == 'IIS':\n",
    "            if isinstance(self.M, int):\n",
    "                print('Ur dataset\\'s all features are same, so GIS algorithm will be used(In this case, IIS == GIS)')\n",
    "                i = 0\n",
    "                while i < self.maxstep:\n",
    "                    self.n_ilter += 1\n",
    "                    i += 1\n",
    "                    delta = self.GIS(X_data, y_data)\n",
    "                    if max(abs(delta)) < self.epsilon:  # 所有的delta都小于阈值时，停止迭代\n",
    "                        break\n",
    "                    self.w += delta\n",
    "            else:        \n",
    "                i = 0\n",
    "                delta = np.random.rand(self.n_fea)\n",
    "                while i < self.maxstep:\n",
    "                    self.n_ilter += 1\n",
    "                    i += 1\n",
    "                    delta = self.IIS(delta, X_data, y_data)\n",
    "                    if max(abs(delta)) < self.epsilon:\n",
    "                        break\n",
    "                    self.w += delta\n",
    "        elif self._algorithms == 'GIS':\n",
    "            if isinstance(self.M, int):\n",
    "                i = 0\n",
    "                while i < self.maxstep:\n",
    "                    self.n_ilter += 1\n",
    "                    i += 1\n",
    "                    delta = self.GIS(X_data, y_data)\n",
    "                    if max(abs(delta)) < self.epsilon:  # 所有的delta都小于阈值时，停止迭代\n",
    "                        break\n",
    "                    self.w += delta\n",
    "            else:\n",
    "                print('Using IIS(different feature numbers among samples)')\n",
    "                i = 0\n",
    "                delta = np.random.rand(self.n_fea)\n",
    "                while i < self.maxstep:\n",
    "                    self.n_ilter += 1\n",
    "                    i += 1\n",
    "                    delta = self.IIS(delta, X_data, y_data)\n",
    "                    if max(abs(delta)) < self.epsilon:\n",
    "                        break\n",
    "                    self.w += delta\n",
    "        elif self._algorithms == 'DFP':\n",
    "            \n",
    "            print('Pleause the another module called MaxEntDFP')\n",
    "                \n",
    "        else:\n",
    "            print('please decide algorithms in IIS&GIS&DFP, defult is GIS/IIS')\n",
    "            return\n",
    "        \n",
    "        print('实际迭代次数{}'.format(self.n_ilter))\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        # 输入x(数组)，返回条件概率最大的标签\n",
    "        py_x = self._py_X(X)\n",
    "        best_label = max(py_x, key=py_x.get) # 注意这个取预测最值的方法\n",
    "        if self._verbose:\n",
    "            print(\"模型权重：{}\".format(self.w))\n",
    "            print(\"模型特征函数量：{}\".format(self.n_fea))\n",
    "        return best_label        \n",
    "    \n",
    "    \n",
    "######################################################################################################################################\n",
    "# DFP\n",
    "######################################################################################################################################\n",
    "\n",
    "\n",
    "class MaxEntDFP:\n",
    "    def __init__(self, epsilon, max_iter=1000, distance=0.01):\n",
    "        \"\"\"\n",
    "        最大熵的DFP算法\n",
    "        :param epsilon: 迭代停止阈值\n",
    "        :param max_iter: 最大迭代次数\n",
    "        :param distance: 一维搜索的长度范围\n",
    "        \"\"\"\n",
    "        print('this algorithms only support str data for now(23.09.19)')\n",
    "        self.distance = distance\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.w = None\n",
    "        self._dataset_X = None\n",
    "        self._dataset_y = None\n",
    "        # 标签集合，相当去去重后的y\n",
    "        self._y = set()\n",
    "        # key为(x,y), value为对应的索引号ID\n",
    "        self._xyID = {}\n",
    "        # key为对应的索引号ID, value为(x,y)\n",
    "        self._IDxy = {}\n",
    "        # 经验分布p(x,y)\n",
    "        self._pxy_dic = defaultdict(int)\n",
    "        # 样本数\n",
    "        self._N = 0\n",
    "        # 特征键值(x,y)的个数\n",
    "        self._n = 0\n",
    "        # 实际迭代次数\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "    # 初始化参数\n",
    "    def init_params(self, X, y):\n",
    "        self._dataset_X = copy.deepcopy(X)\n",
    "        self._dataset_y = copy.deepcopy(y)\n",
    "        self._N = X.shape[0]\n",
    "\n",
    "        for i in range(self._N):\n",
    "            xi, yi = X[i], y[i]\n",
    "            self._y.add(yi)\n",
    "            for _x in xi:\n",
    "                self._pxy_dic[(_x, yi)] += 1\n",
    "\n",
    "        self._n = len(self._pxy_dic)\n",
    "        # 初始化权重w\n",
    "        self.w = np.zeros(self._n)\n",
    "\n",
    "        for i, xy in enumerate(self._pxy_dic):\n",
    "            self._pxy_dic[xy] /= self._N\n",
    "            self._xyID[xy] = i\n",
    "            self._IDxy[i] = xy\n",
    "\n",
    "    def calc_zw(self, X, w):\n",
    "        \"\"\"书中第100页公式6.23，计算Zw(x)\"\"\"\n",
    "        zw = 0.0\n",
    "        for y in self._y:\n",
    "            zw += self.calc_ewf(X, y, w)\n",
    "        return zw\n",
    "\n",
    "    def calc_ewf(self, X, y, w):\n",
    "        \"\"\"书中第100页公式6.22，计算分子\"\"\"\n",
    "        sum_wf = self.calc_wf(X, y, w)\n",
    "        return np.exp(sum_wf)\n",
    "\n",
    "    def calc_wf(self, X, y, w):\n",
    "        sum_wf = 0.0\n",
    "        for x in X:\n",
    "            if (x, y) in self._pxy_dic:\n",
    "                sum_wf += w[self._xyID[(x, y)]]\n",
    "        return sum_wf\n",
    "\n",
    "    def calc_pw_yx(self, X, y, w):\n",
    "        \"\"\"计算Pw(y|x)\"\"\"\n",
    "        return self.calc_ewf(X, y, w) / self.calc_zw(X, w)\n",
    "\n",
    "    def calc_f(self, w):\n",
    "        \"\"\"计算f(w)\"\"\"\n",
    "        fw = 0.0\n",
    "        for i in range(self._n):\n",
    "            x, y = self._IDxy[i]\n",
    "            for dataset_X in self._dataset_X:\n",
    "                if x not in dataset_X:\n",
    "                    continue\n",
    "                fw += np.log(self.calc_zw(x, w)) - \\\n",
    "                    self._pxy_dic[(x, y)] * self.calc_wf(dataset_X, y, w)\n",
    "\n",
    "        return fw\n",
    "\n",
    "    # DFP算法\n",
    "    def fit(self, X, y):\n",
    "        self.init_params(X, y)\n",
    "\n",
    "        def calc_dfw(i, w):\n",
    "            \"\"\"计算书中第107页的拟牛顿法f(w)的偏导\"\"\"\n",
    "\n",
    "            def calc_ewp(i, w):\n",
    "                \"\"\"计算偏导左边的公式\"\"\"\n",
    "                ep = 0.0\n",
    "                x, y = self._IDxy[i]\n",
    "                for dataset_X in self._dataset_X:\n",
    "                    if x not in dataset_X:\n",
    "                        continue\n",
    "                    ep += self.calc_pw_yx(dataset_X, y, w) / self._N\n",
    "                return ep\n",
    "\n",
    "            def calc_ep(i):\n",
    "                \"\"\"计算关于经验分布P(x,y)的期望值\"\"\"\n",
    "                (x, y) = self._IDxy[i]\n",
    "                return self._pxy_dic[(x, y)]\n",
    "\n",
    "            return calc_ewp(i, w) - calc_ep(i)\n",
    "\n",
    "        # 算出g(w)，是n*1维矩阵\n",
    "        def calc_gw(w):\n",
    "            return np.array([[calc_dfw(i, w) for i in range(self._n)]]).T\n",
    "\n",
    "        # （1）初始正定对称矩阵，单位矩阵\n",
    "        Gk = np.array(np.eye(len(self.w), dtype=float))\n",
    "\n",
    "        # （2）计算g(w0)\n",
    "        w = self.w\n",
    "        gk = calc_gw(w)\n",
    "        # 判断gk的范数是否小于阈值\n",
    "        if np.linalg.norm(gk, ord=2) < self.epsilon:\n",
    "            self.w = w\n",
    "            return\n",
    "\n",
    "        k = 0\n",
    "        for _ in range(self.max_iter):\n",
    "            # （3）计算pk\n",
    "            pk = -Gk.dot(gk)\n",
    "\n",
    "            # 梯度方向的一维函数\n",
    "            def _f(x):\n",
    "                z = w + np.dot(x, pk).T[0]\n",
    "                return self.calc_f(z)\n",
    "\n",
    "            # （4）进行一维搜索，找到使得函数最小的lambda\n",
    "            _lambda = fminbound(_f, -self.distance, self.distance)\n",
    "\n",
    "            delta_k = _lambda * pk\n",
    "            # （5）更新权重\n",
    "            w += delta_k.T[0]\n",
    "\n",
    "            # （6）计算gk+1\n",
    "            gk1 = calc_gw(w)\n",
    "            # 判断gk1的范数是否小于阈值\n",
    "            if np.linalg.norm(gk1, ord=2) < self.epsilon:\n",
    "                self.w = w\n",
    "                break\n",
    "            # 根据DFP算法的迭代公式（附录B.24公式）计算Gk\n",
    "            yk = gk1 - gk\n",
    "            Pk = delta_k.dot(delta_k.T) / (delta_k.T.dot(yk))\n",
    "            Qk = Gk.dot(yk).dot(yk.T).dot(Gk) / (yk.T.dot(Gk).dot(yk)) * (-1)\n",
    "            Gk = Gk + Pk + Qk\n",
    "            gk = gk1\n",
    "\n",
    "            # （7）置k=k+1\n",
    "            k += 1\n",
    "\n",
    "        self.w = w\n",
    "        self.n_iter_ = k\n",
    "\n",
    "    def predict(self, x):\n",
    "        result = {}\n",
    "        for y in self._y:\n",
    "            prob = self.calc_pw_yx(x, y, self.w)\n",
    "            result[y] = prob\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# test function\n",
    "def main(algorithms=''):\n",
    "    if algorithms == 'DFP':\n",
    "        # 训练数据集\n",
    "        dataset = np.array([['no', 'sunny', 'hot', 'high', 'FALSE'],\n",
    "                            ['no', 'sunny', 'hot', 'high', 'TRUE'],\n",
    "                            ['yes', 'overcast', 'hot', 'high', 'FALSE'],\n",
    "                            ['yes', 'rainy', 'mild', 'high', 'FALSE'],\n",
    "                            ['yes', 'rainy', 'cool', 'normal', 'FALSE'],\n",
    "                            ['no', 'rainy', 'cool', 'normal', 'TRUE'],\n",
    "                            ['yes', 'overcast', 'cool', 'normal', 'TRUE'],\n",
    "                            ['no', 'sunny', 'mild', 'high', 'FALSE'],\n",
    "                            ['yes', 'sunny', 'cool', 'normal', 'FALSE'],\n",
    "                            ['yes', 'rainy', 'mild', 'normal', 'FALSE'],\n",
    "                            ['yes', 'sunny', 'mild', 'normal', 'TRUE'],\n",
    "                            ['yes', 'overcast', 'mild', 'high', 'TRUE'],\n",
    "                            ['yes', 'overcast', 'hot', 'normal', 'FALSE'],\n",
    "                            ['no', 'rainy', 'mild', 'high', 'TRUE']])\n",
    "\n",
    "        X_train = dataset[:, 1:]\n",
    "        y_train = dataset[:, 0]\n",
    "\n",
    "        mae = MaxEntDFP(epsilon=1e-4, max_iter=1000, distance=0.01)\n",
    "        # 训练模型\n",
    "        mae.fit(X_train, y_train)\n",
    "        print(\"模型训练迭代次数：{}次\".format(mae.n_iter_))\n",
    "        print(\"模型权重：{}\".format(mae.w))\n",
    "\n",
    "        result = mae.predict(['overcast', 'mild', 'high', 'FALSE'])\n",
    "        print(\"预测结果：\", result)\n",
    "    else:\n",
    "        print('running IIS/GIS')\n",
    "        from sklearn.datasets import load_iris\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        data = load_iris()\n",
    "        X_data = data['data']\n",
    "        y_data = data['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=0)\n",
    "\n",
    "        ME = MaxEnt(algorithms='')\n",
    "        a = ME.fit(X_train, y_train)\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        for X, y in zip(X_test, y_test):\n",
    "            if ME.predict(X) == y:\n",
    "                score += 1\n",
    "        print(score / len(y_test) * 100)\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main('')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tool use a special way to get more eigenfunction\n",
      "实际迭代次数200\n",
      "81.57894736842105\n"
     ]
    }
   ],
   "source": [
    "# IIS/GIS\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "X_data = data['data']\n",
    "y_data = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=0)\n",
    "\n",
    "ME = MaxEnt(algorithms='')\n",
    "a = ME.fit(X_train, y_train)\n",
    "\n",
    "score = 0\n",
    "\n",
    "for X, y in zip(X_test, y_test):\n",
    "    if ME.predict(X) == y:\n",
    "        score += 1\n",
    "print(score / len(y_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this tool use a special way to get more eigenfunction\n",
      "实际迭代次数200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DFP\n",
    "dataset = np.array([['no', 'sunny', 'hot', 'high', 'FALSE'],\n",
    "                    ['no', 'sunny', 'hot', 'high', 'TRUE'],\n",
    "                    ['yes', 'overcast', 'hot', 'high', 'FALSE'],\n",
    "                    ['yes', 'rainy', 'mild', 'high', 'FALSE'],\n",
    "                    ['yes', 'rainy', 'cool', 'normal', 'FALSE'],\n",
    "                    ['no', 'rainy', 'cool', 'normal', 'TRUE'],\n",
    "                    ['yes', 'overcast', 'cool', 'normal', 'TRUE'],\n",
    "                    ['no', 'sunny', 'mild', 'high', 'FALSE'],\n",
    "                    ['yes', 'sunny', 'cool', 'normal', 'FALSE'],\n",
    "                    ['yes', 'rainy', 'mild', 'normal', 'FALSE'],\n",
    "                    ['yes', 'sunny', 'mild', 'normal', 'TRUE'],\n",
    "                    ['yes', 'overcast', 'mild', 'high', 'TRUE'],\n",
    "                    ['yes', 'overcast', 'hot', 'normal', 'FALSE'],\n",
    "                    ['no', 'rainy', 'mild', 'high', 'TRUE']])\n",
    "\n",
    "X_train_new = dataset[:, 1:]\n",
    "y_train_new = dataset[:, 0]\n",
    "\n",
    "ME = MaxEnt(algorithms='')\n",
    "ME.fit(X_train_new, y_train_new)\n",
    "\n",
    "ME.predict(['overcast', 'mild', 'high', 'FALSE'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试下来感觉IIS/GIS算法好用......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
