{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a70aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "# 用sklearn构建决策树，使用C4.5算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669b65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      " |         1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated:: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c83797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeRegressor in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeRegressor(sklearn.base.RegressorMixin, BaseDecisionTree)\n",
      " |  DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree regressor.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"squared_error\", \"friedman_mse\", \"absolute_error\",             \"poisson\"}, default=\"squared_error\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"squared_error\" for the mean squared error, which is equal to\n",
      " |      variance reduction as feature selection criterion and minimizes the L2\n",
      " |      loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      " |      mean squared error with Friedman's improvement score for potential\n",
      " |      splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      " |      the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      " |      uses reduction in Poisson deviance to find splits.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |          Poisson deviance criterion.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the\n",
      " |      (normalized) total reduction of the criterion brought\n",
      " |      by that feature. It is also known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      " |         1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier : A decision tree classifier.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_diabetes\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeRegressor\n",
      " |  >>> X, y = load_diabetes(return_X_y=True)\n",
      " |  >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      " |  >>> cross_val_score(regressor, X, y, cv=10)\n",
      " |  ...                    # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
      " |         0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      " |          ``order='C'`` for maximum efficiency.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated:: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeRegressor\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bce6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先传入训练集\n",
    "features = [\"年龄\", \"有工作\", \"有自己的房子\", \"信贷情况\"]\n",
    "X_train = pd.DataFrame([\n",
    "    [\"青年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"青年\", \"否\", \"否\", \"好\"],\n",
    "    [\"青年\", \"是\", \"否\", \"好\"],\n",
    "    [\"青年\", \"是\", \"是\", \"一般\"],\n",
    "    [\"青年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"中年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"中年\", \"否\", \"否\", \"好\"],\n",
    "    [\"中年\", \"是\", \"是\", \"好\"],\n",
    "    [\"中年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"中年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"是\", \"好\"],\n",
    "    [\"老年\", \"是\", \"否\", \"好\"],\n",
    "    [\"老年\", \"是\", \"否\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"否\", \"一般\"]\n",
    "])\n",
    "y_train = pd.DataFrame([\"否\", \"否\", \"是\", \"是\", \"否\",\n",
    "                        \"否\", \"否\", \"是\", \"是\", \"是\",\n",
    "                        \"是\", \"是\", \"是\", \"是\", \"否\"])\n",
    "class_names = [str(k) for k in np.unique(y_train)]\n",
    "\n",
    "# 数据预处理\n",
    "# 在这里要注意，我们这里构建的是一个分类树，意味着我们传入的特征是字符串类型的\n",
    "# 为了处理方便，我们需要首先将特征进行预处理，也就是进行一个“标准化”\n",
    "le_x = preprocessing.LabelEncoder()\n",
    "le_x.fit(np.unique(X_train)) # 将每个特征与数字匹配\n",
    "# 此处的apply其实和R语言里面的一模一样，不过可以指定按照行或者列来进行apply\n",
    "X_train = X_train.apply(le_x.transform) # transform函数就是将字符串特征转化为数字，Transform labels to normalized encoding.\n",
    "\n",
    "# 进行模型构建\n",
    "# sklearn使用的算法默认是改进后的CART算法，其实和C4.5基本类似，只不过默认使用gini作为标准，在构建模型的时候也可以选entropy，这样就是C4.5\n",
    "# criterion = \"entropy\"\n",
    "model_tree = DecisionTreeClassifier()\n",
    "# 训练模型\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# 导出决策树的可视化文件，文件格式是dot\n",
    "dot_data = tree.export_graphviz(model_tree, out_file=None,\n",
    "                                feature_names=features,\n",
    "                                class_names=class_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "# 使用graphviz包，对决策树进行展示，没装\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e19fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- 有自己的房子 <= 3.00\n",
      "|   |--- 有工作 <= 3.00\n",
      "|   |   |--- class: 否\n",
      "|   |--- 有工作 >  3.00\n",
      "|   |   |--- class: 是\n",
      "|--- 有自己的房子 >  3.00\n",
      "|   |--- class: 是\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这个就是用sklearn自带的导出工具\n",
    "tree_text = tree.export_text(model_tree, feature_names = features)\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f47c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2  3\n",
       "0   6  2  2  0\n",
       "1   6  2  2  3\n",
       "2   6  4  2  3\n",
       "3   6  4  4  0\n",
       "4   6  2  2  0\n",
       "5   1  2  2  0\n",
       "6   1  2  2  3\n",
       "7   1  4  4  3\n",
       "8   1  2  4  7\n",
       "9   1  2  4  7\n",
       "10  5  2  4  7\n",
       "11  5  2  4  3\n",
       "12  5  4  2  3\n",
       "13  5  4  2  7\n",
       "14  5  2  2  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "# 可以发现，标准化后的X_train其中特征就完全变成了数字\n",
    "# 其实就可以当成回归器里面那种连续型变量来处理\n",
    "# sklearn官方文档里面也说了，他们用的这种改进型CART算法不支持分类变量，可能是由于这个关系所以要这样处理数据（？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccafed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.1\n",
    "# 用平方误差准则，生成一个二叉回归树\n",
    "# 这里是直接用CART算法来进行生成的，并且没有进行剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c125cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).T\n",
    "y = np.array([4.50, 4.75, 4.91, 5.34, 5.80, 7.05, 7.90, 8.23, 8.70, 9.00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b41b1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 没剪枝\n",
    "Regressor_Tree = DecisionTreeRegressor()\n",
    "Regressor_Tree.fit(train_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018591fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- X <= 5.50\n",
      "|   |--- X <= 3.50\n",
      "|   |   |--- X <= 1.50\n",
      "|   |   |   |--- value: [4.50]\n",
      "|   |   |--- X >  1.50\n",
      "|   |   |   |--- X <= 2.50\n",
      "|   |   |   |   |--- value: [4.75]\n",
      "|   |   |   |--- X >  2.50\n",
      "|   |   |   |   |--- value: [4.91]\n",
      "|   |--- X >  3.50\n",
      "|   |   |--- X <= 4.50\n",
      "|   |   |   |--- value: [5.34]\n",
      "|   |   |--- X >  4.50\n",
      "|   |   |   |--- value: [5.80]\n",
      "|--- X >  5.50\n",
      "|   |--- X <= 7.50\n",
      "|   |   |--- X <= 6.50\n",
      "|   |   |   |--- value: [7.05]\n",
      "|   |   |--- X >  6.50\n",
      "|   |   |   |--- value: [7.90]\n",
      "|   |--- X >  7.50\n",
      "|   |   |--- X <= 8.50\n",
      "|   |   |   |--- value: [8.23]\n",
      "|   |   |--- X >  8.50\n",
      "|   |   |   |--- X <= 9.50\n",
      "|   |   |   |   |--- value: [8.70]\n",
      "|   |   |   |--- X >  9.50\n",
      "|   |   |   |   |--- value: [9.00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree.export_text(Regressor_Tree, feature_names = 'X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be4e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- X <= 5.50\n",
      "|   |--- X <= 3.50\n",
      "|   |   |--- value: [4.72]\n",
      "|   |--- X >  3.50\n",
      "|   |   |--- value: [5.57]\n",
      "|--- X >  5.50\n",
      "|   |--- X <= 7.50\n",
      "|   |   |--- X <= 6.50\n",
      "|   |   |   |--- value: [7.05]\n",
      "|   |   |--- X >  6.50\n",
      "|   |   |   |--- value: [7.90]\n",
      "|   |--- X >  7.50\n",
      "|   |   |--- X <= 8.50\n",
      "|   |   |   |--- value: [8.23]\n",
      "|   |   |--- X >  8.50\n",
      "|   |   |   |--- value: [8.85]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Total Impurity vs effective alpha for training set')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 剪枝\n",
    "# 其实按照这个alpha参数设置，基本上和预剪枝的那种没啥区别，就是切分点不一样，但是最终的叶子结点是一样的\n",
    "Regressor_Tree = DecisionTreeRegressor(ccp_alpha = 0.02)\n",
    "Regressor_Tree.fit(train_X, y)\n",
    "print(tree.export_text(Regressor_Tree, feature_names = 'X'))\n",
    "\n",
    "# 剪枝过程中，sklearn提供了cost_complexity_pruning_path来让你看到每一步剪枝生成的alpha以及最佳子树\n",
    "pruning_path = Regressor_Tree.cost_complexity_pruning_path(train_X, y)\n",
    "# 其实就像fit拟合一样，用训练数据集放进去剪枝，然后就能得到子树序列的alpha以及对应的欠拟合情况\n",
    "alpha, impurities = pruning_path.ccp_alphas, pruning_path.impurities\n",
    "alpha\n",
    "\n",
    "# 画图展示\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha[:-1], impurities[:-1], marker = \"o\", drawstyle = \"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "# 随着alpha增大，欠拟合度也在不断增大\n",
    "# 当然如果这里有测试集，就可以结合测试集的数据，来看不同alpha下的测试集/训练集准确率，从而选择合适的alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e5b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb7f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
